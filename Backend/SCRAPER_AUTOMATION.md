# Automatyzacja Scraper√≥w - Dokumentacja

## PrzeglƒÖd

System automatycznego scrapingu danych o lekach z dw√≥ch ≈∫r√≥de≈Ç:
- **GIF** (G≈Ç√≥wny Inspektorat Farmaceutyczny) - wycofania i zawieszenia
- **URPL** (UrzƒÖd Rejestracji Produkt√≥w Leczniczych) - nowe rejestracje

## Harmonogram

### Automatyczne uruchomienie
- **Godzina**: Codziennie o **6:00 rano** (strefa czasowa: Europe/Warsaw)
- **Sprawdzanie**: Je≈õli scraping zosta≈Ç ju≈º dzisiaj wykonany - pomija
- **≈πr√≥d≈Ça danych**: Oba scrapery (GIF + URPL) uruchamiane r√≥wnocze≈õnie

### Uruchomienie przy starcie
- **Start aplikacji**: System automatycznie sprawdza czy scraping zosta≈Ç wykonany dzisiaj
- **Je≈õli nie**: Uruchamia oba scrapery
- **Je≈õli tak**: Pomija i czeka na nastƒôpny zaplanowany czas

## Architektura

### Komponenty

1. **Celery Worker** - wykonuje zadania scrapingu w tle
2. **Celery Beat** - scheduler (harmonogram) uruchamiajƒÖcy zadania o 6:00
3. **Redis** - broker kolejek zada≈Ñ
4. **PostgreSQL** - przechowuje dane i historiƒô zada≈Ñ

### Kontenery Docker

```yaml
- api_hackathon      # Django API
- celery_worker      # Wykonawca zada≈Ñ
- celery_beat        # Harmonogram
- redis              # Broker
- db_hackathon       # Baza danych
```

## Zadania Celery

### 1. `run_daily_scraping`
G≈Ç√≥wne zadanie uruchamiane codziennie o 6:00.

**Funkcjonalno≈õƒá:**
- Sprawdza czy scraping zosta≈Ç ju≈º wykonany dzisiaj
- Uruchamia oba scrapery (GIF + URPL)
- Loguje wyniki
- Zwraca statystyki

**Harmonogram:**
```python
'scrape-daily-at-6am': {
    'task': 'scraper.tasks.run_daily_scraping',
    'schedule': crontab(hour=6, minute=0),
}
```

### 2. `check_and_run_scraping`
Sprawdza czy scraping jest potrzebny i uruchamia je≈õli tak.

**U≈ºycie:**
- Wywo≈Çywane przy starcie aplikacji
- Mo≈ºna wywo≈Çaƒá rƒôcznie w dowolnym momencie

### 3. `run_gif_scraper`
Uruchamia tylko scraper GIF (wycofania/zawieszenia).

### 4. `run_urpl_scraper`
Uruchamia tylko scraper URPL (rejestracje).

## Rƒôczne uruchamianie

### Uruchomienie wszystkich scraper√≥w
```bash
# W kontenerze Docker
docker-compose exec api_hackathon python manage.py do_scrape --show-details

# Przez Celery (asynchronicznie)
docker-compose exec api_hackathon python manage.py shell -c \
  "from scraper.tasks import check_and_run_scraping; check_and_run_scraping.delay()"
```

### Uruchomienie pojedynczego scrapera
```bash
# GIF scraper
docker-compose exec api_hackathon python manage.py scrape_gif

# URPL scraper
docker-compose exec api_hackathon python manage.py scrape_urpl
```

### Sprawdzenie czy scraping jest potrzebny
```bash
docker-compose exec api_hackathon python manage.py check_scraping
```

## Monitorowanie

### Logi Celery Worker
```bash
docker-compose logs celery_worker -f
```

### Logi Celery Beat (harmonogram)
```bash
docker-compose logs celery_beat -f
```

### Status wszystkich kontener√≥w
```bash
docker-compose ps
```

### Sprawdzenie wykonanych zada≈Ñ
Wejd≈∫ do Django Admin ‚Üí Celery Results ‚Üí Task Results

## Konfiguracja

### Zmiana godziny uruchomienia

Edytuj `/Backend/api/celery.py`:

```python
app.conf.beat_schedule = {
    'scrape-daily-at-6am': {
        'task': 'scraper.tasks.run_daily_scraping',
        'schedule': crontab(hour=8, minute=30),  # Zmie≈Ñ na 8:30
    },
}
```

Nastƒôpnie:
```bash
docker-compose restart celery_beat
```

### Zmiana strefy czasowej

Edytuj `/Backend/api/celery.py`:

```python
app.conf.timezone = 'America/New_York'  # Zmie≈Ñ strefƒô
```

## Bezpiecze≈Ñstwo duplikat√≥w

System zapobiega duplikatom na 2 poziomach:

### 1. Constraint w bazie danych
```python
models.UniqueConstraint(
    fields=['event_type', 'drug_name', 'source'],
    name='unique_event_drug_source'
)
```

### 2. Sprawdzanie przed dodaniem
Scraper sprawdza czy rekord ju≈º istnieje przed pr√≥bƒÖ dodania.

## Szczeg√≥≈Çy techniczne

### Broker: Redis
- **URL**: `redis://redis:6379/0`
- **Port**: 6379
- **Persistencja**: Volume `redis_data`

### Baza danych: PostgreSQL
- **Host**: `db_hackathon`
- **Port**: 5432
- **Persistencja**: Volume `postgres_data`

### Celery Configuration
- **Result Backend**: Django DB
- **Serializer**: JSON
- **Timezone**: Europe/Warsaw
- **Scheduler**: DatabaseScheduler (django-celery-beat)

## Troubleshooting

### Problem: Zadania nie sƒÖ wykonywane

1. Sprawd≈∫ czy Celery Worker dzia≈Ça:
```bash
docker-compose ps | grep celery_worker
```

2. Sprawd≈∫ logi workera:
```bash
docker-compose logs celery_worker --tail=50
```

3. Sprawd≈∫ po≈ÇƒÖczenie z Redis:
```bash
docker-compose exec redis redis-cli ping
# Powinno zwr√≥ciƒá: PONG
```

### Problem: Harmonogram siƒô nie wykonuje

1. Sprawd≈∫ czy Celery Beat dzia≈Ça:
```bash
docker-compose ps | grep celery_beat
```

2. Sprawd≈∫ harmonogram w bazie:
```bash
docker-compose exec api_hackathon python manage.py shell -c \
  "from django_celery_beat.models import PeriodicTask; print(PeriodicTask.objects.all())"
```

### Problem: Duplikaty sƒÖ dodawane

1. Sprawd≈∫ constraint w bazie:
```bash
docker-compose exec db_hackathon psql -U HackUser -d HackDB -c \
  "SELECT conname FROM pg_constraint WHERE conname='unique_event_drug_source';"
```

2. Je≈õli nie ma - uruchom migracje:
```bash
docker-compose exec api_hackathon python manage.py migrate
```

## Przyk≈Çadowy output

```
[2025-10-24 14:34:02] üöÄ Starting daily scraping task...
[2025-10-24 14:34:02] üìä No records found for today (2025-10-24). Starting scrapers...
[2025-10-24 14:34:02] üìã Running GIF scraper...
[2025-10-24 14:34:02] ‚úÖ Created: Benlek - Wycofanie z obrotu
[2025-10-24 14:34:02] ‚úÖ GIF scraper completed: 3 new records
[2025-10-24 14:34:02] üíä Running URPL scraper...
[2025-10-24 14:34:02] ‚úÖ Created: Natrii chloridum - REGISTRATION
[2025-10-24 14:34:02] ‚úÖ URPL scraper completed: 18 new records
[2025-10-24 14:34:02] üéâ Daily scraping completed! Total new records: 21
```

## Referencje

- [Celery Documentation](https://docs.celeryproject.org/)
- [Django Celery Beat](https://django-celery-beat.readthedocs.io/)
- [Redis](https://redis.io/documentation)

